{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lee9ogEeD6Iu"
      },
      "source": [
        "# Applying Super-Resolution to Sentinel-2 Imagery\n",
        "### Alexander Vu and Benjamin Gaskill\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zr6PdHjEGtb"
      },
      "source": [
        "# **Description**\n",
        "### The goal of our project is to research, adapt, and fine-tune super-resolution deep learning frameworks to resample Sentinel-2 imagery from its native resolution of 10 meters up to 1.5 meters per pixel. Our main focus is on the WorldStrat super-resolution model, which we tested using stacks of input imagery for 6 selected sites across Zambia. The model accepts a multi-temporal stack of 8 images, along with all 12 bands of Sentinel 2 per study site. However, due to time constraints and Google Colab processing limits, we ran the inference pipeline only for Sites 0 and 1. The model outputs the original 10-meter bands upscaled to 1.5 meters, namely the red, blue, and green bands.\n",
        "\n",
        "### We developed an imagery acquisition and preprocessing script and fed the input images into our inference pipeline. To assess the performance of the model, the super-resolution outputs were compared against the original Sentinel 2 images, as well as high-resolution Planet imagery located at the same test sites. We hope that our model can be utilized by the Agricultural Impacts Research Group to serve as a complement to paid high-resolution imagery such as Planet.  \n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GrRFtLNEaEz"
      },
      "source": [
        "# **Division of Labor**\n",
        "### Ben developed a data acquisition and pre-processing pipeline for Sentinel 2 imagery. This notebook connects to Google Earth Engine and samples 6 bounding boxes within Zambia. 3 samples represent mixed urban areas, and 3 samples represent rural areas. By sampling geographically diverse areas, we hoped to capture finer-grain details for our model to perform more effectively. The script processes cloud-free composites for 2024 by selecting imagery with less than 5 percent cloud coverage and masking cloudy pixels with the Scene Classification Layer. The script also uses Sentinel-2 footprints to ensure that each study area falls fully within one footprint and that there is valid data for the entire extent. The layers are then exported as GeoTiff files. Ben also worked on modifying and improving the outputs of the inference pipeline, namely the testing of different normalization strategies, creating Cloud-Optimized Geotiffs, and merging the chips into one final image. He developed the visualization notebook, which displays the results in leaflet maps.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### Alex tested the WorldStrat super-resolution model using the pre-existing data in order to understand the input requirements. He also developed the inference pipeline, which chips the input images, adapts them to the modelâ€™s input, applies normalization, and builds the full super-resolution image.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J1tzHU0mOZv"
      },
      "source": [
        "# **Key Findings**\n",
        "### We have found that the model performs reasonably well on our first study site. Most of the time spent on the project went into development and debugging, especially with applying normalization to the output. Multiple iterations of the image acquisition and processing pipeline notebooks were created, as well as many versions of the Super-Resolved output.\n",
        "\n",
        "### We decided to include 2 versions of the final output, in which one is normalized per-chip and the other is normalized across the chips. We felt it useful to show both results, highlighting the strengths and weaknesses of each.  \n",
        "\n",
        "### With the study sites we tested, we found that normalization across chips and bands proved to be more effective in representing the original spectral characteristics of the original Sentinel 2 images. Our first tests with per-band / per-chip normalization is overly green, with saturation and over-prediction of fields, especially in urban areas and barren lands. Our second tests with cross-chip / cross-band normalization appears significantly better at small scales, while certain areas suffer the same issues when zooming in. In the future, we would like to further refine the model by testing in smaller but more geographically diverse regions. This would include a more careful selection of imagery, possibly from dates that are closer together. \n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
